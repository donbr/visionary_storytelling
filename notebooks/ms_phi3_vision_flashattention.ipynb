{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbr/visionary_storytelling/blob/main/notebooks/ms_phi3_vision_flashattention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FLASH ATTENTION... and the Phi3 Vision Language Model\n",
        "\n",
        "- see the [model card](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) for additional detail\n",
        "- as the current Phi 3 Vision model requires Flash Attention it will only run on very specific GPUs\n",
        "  - on Google Colab, it will run on A100 or L4\n",
        "  - will also run on T4 with an additional model parameter\n",
        "  - refer to [model card README](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct#running-on-windows-or-without-flash-attention) for additional detail on the fix\n",
        "- Currently bleeding edge - Phi3 Vision model card directs devs to install  transformers using lates updates from GitHub"
      ],
      "metadata": {
        "id": "FBQqObAuV-vx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Python Dependencies"
      ],
      "metadata": {
        "id": "s16yW6o0l87U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "CaM6YGddUx_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU flash_attn accelerate"
      ],
      "metadata": {
        "id": "AzzvECrOVcGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries"
      ],
      "metadata": {
        "id": "1t3ofIR2mHhX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfJNXElhUaAx"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoProcessor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face model configuration\n",
        "- Load the Phi-3 Vision model and processor from Hugging Face"
      ],
      "metadata": {
        "id": "9OlACINKmNqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face model configuration\n",
        "\n",
        "model_id = \"microsoft/Phi-3-vision-128k-instruct\"\n",
        "\n",
        "# standard model config\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\")\n",
        "\n",
        "# model config workaround for flash_attention_2 issues on older GPU architectures\n",
        "# refer to [model card README](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct#running-on-windows-or-without-flash-attention) for additional detail\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation=\"eager\")\n",
        "\n",
        "# processor configuration\n",
        "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "uv-ADG6eaLLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the Model Architecture\n",
        "\n",
        "- Print the model architecture to understand its structure"
      ],
      "metadata": {
        "id": "NLR8po3xmjPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the Model Architecture\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "eb8Y8bI6WjTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the input for the Vision model"
      ],
      "metadata": {
        "id": "xlLfWy5qmuU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the messages to be processed by the model\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image?\"},\n",
        "\n",
        "]\n",
        "\n",
        "# Load the image from a URL\n",
        "url = \"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/15b17bf0-fb1a-4fb2-b952-beee07706068/width=832/00088-3178799381.jpeg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Process the messages and image to create input tensors\n",
        "prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\")\n"
      ],
      "metadata": {
        "id": "W7kFWEpNWh6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Run Vision model inference\n"
      ],
      "metadata": {
        "id": "YrfgSBNvm-cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define generation arguments for the model's response\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "# Generate the response from the Vision model\n",
        "generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)\n",
        "\n",
        "# Remove input tokens from the generated response to get the actual output tokens\n",
        "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
        "\n",
        "# Decode the response to get the final text output\n",
        "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n"
      ],
      "metadata": {
        "id": "o3Evw-mJYFDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discover the Model's Insights!"
      ],
      "metadata": {
        "id": "4vHLaF_SnV3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "tWyc_b-PZ2fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(image)"
      ],
      "metadata": {
        "id": "OdNQS080raLx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}